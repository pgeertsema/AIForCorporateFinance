{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "### import packages ###\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import lightgbm as lgb"
      ],
      "metadata": {
        "id": "dtiFuIU7r2TN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Load the data ###\n",
        "\n",
        "url = \"https://github.com/pgeertsema/AIForCorporateFinance/raw/refs/heads/main/jkp_seo.dta\"\n",
        "df = pd.read_stata(url)\n",
        "\n",
        "# Reduce fragmentation post read_stata()\n",
        "df = df.copy()"
      ],
      "metadata": {
        "id": "ZHebAK5ir7Nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Preprocess the data ###\n",
        "\n",
        "# Convert seo_next_yr to binary (assume > 0 as 1, else 0)\n",
        "df['seo_next_yr'] = (df['seo_next_yr'] > 0).astype(int)\n",
        "\n",
        "# Extract year as an integer into nyear\n",
        "df['nyear'] = df['year'].dt.year\n",
        "\n",
        "# Encode ff49 as categorical\n",
        "df['ff49'] = df['ff49'].astype('category')\n",
        "\n",
        "# Features: all variables from \"bidask\" to \"debt_at\"\n",
        "features = df.loc[:, 'bidask':'debt_at'].columns.tolist()\n",
        "\n",
        "# Target: seo_next_yr\n",
        "target = 'seo_next_yr'\n"
      ],
      "metadata": {
        "id": "KgvXYVaSr_0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Split data into train/val/test ###\n",
        "\n",
        "# Test: last 5 years\n",
        "test_years = df['nyear'].max() - 4\n",
        "val_years = df['nyear'].max() - 7\n",
        "\n",
        "test_data  = df[ df['nyear'] >= test_years]\n",
        "val_data   = df[(df['nyear'] >= val_years) & (df['nyear'] < test_years)]\n",
        "train_data = df[ df['nyear'] <  val_years]\n",
        "\n",
        "# Split into features and target\n",
        "X_train, y_train = train_data[features], train_data[target]\n",
        "X_val, y_val = val_data[features], val_data[target]\n",
        "X_test, y_test = test_data[features], test_data[target]\n"
      ],
      "metadata": {
        "id": "ap19ImKrsK-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###  Train LightGBM Classifier ###\n",
        "\n",
        "# Create a LightGBM Dataset\n",
        "train_set = lgb.Dataset(X_train, label=y_train)\n",
        "val_set = lgb.Dataset(X_val, label=y_val, reference=train_set)\n",
        "\n",
        "# Specify the model\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'binary_error',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 31,\n",
        "    'learning_rate': 0.1,\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "# Train the model with early stopping\n",
        "model = lgb.train(\n",
        "    params,\n",
        "    train_set,\n",
        "    num_boost_round=1000,\n",
        "    valid_sets=[train_set, val_set],\n",
        "    callbacks=[\n",
        "        lgb.early_stopping(stopping_rounds=50),\n",
        "        lgb.log_evaluation(1)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jg98b8kSsQv1",
        "outputId": "8d3ee4a7-1a7e-4915-fc26-8c5aa3c0fe6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 1153, number of negative: 9678\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016167 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 29119\n",
            "[LightGBM] [Info] Number of data points in the train set: 10831, number of used features: 115\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.106454 -> initscore=-2.127488\n",
            "[LightGBM] [Info] Start training from score -2.127488\n",
            "[1]\ttraining's binary_error: 0.106454\tvalid_1's binary_error: 0.0996252\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "[2]\ttraining's binary_error: 0.106454\tvalid_1's binary_error: 0.0996252\n",
            "[3]\ttraining's binary_error: 0.106454\tvalid_1's binary_error: 0.0996252\n",
            "[4]\ttraining's binary_error: 0.106454\tvalid_1's binary_error: 0.0996252\n",
            "[5]\ttraining's binary_error: 0.106269\tvalid_1's binary_error: 0.0996252\n",
            "[6]\ttraining's binary_error: 0.105438\tvalid_1's binary_error: 0.0996252\n",
            "[7]\ttraining's binary_error: 0.105161\tvalid_1's binary_error: 0.0996252\n",
            "[8]\ttraining's binary_error: 0.10433\tvalid_1's binary_error: 0.0990006\n",
            "[9]\ttraining's binary_error: 0.103499\tvalid_1's binary_error: 0.0996252\n",
            "[10]\ttraining's binary_error: 0.102668\tvalid_1's binary_error: 0.0993129\n",
            "[11]\ttraining's binary_error: 0.10156\tvalid_1's binary_error: 0.098376\n",
            "[12]\ttraining's binary_error: 0.0997138\tvalid_1's binary_error: 0.0986883\n",
            "[13]\ttraining's binary_error: 0.0986982\tvalid_1's binary_error: 0.098376\n",
            "[14]\ttraining's binary_error: 0.0974056\tvalid_1's binary_error: 0.0990006\n",
            "[15]\ttraining's binary_error: 0.0956514\tvalid_1's binary_error: 0.0993129\n",
            "[16]\ttraining's binary_error: 0.0941741\tvalid_1's binary_error: 0.0996252\n",
            "[17]\ttraining's binary_error: 0.0933432\tvalid_1's binary_error: 0.10025\n",
            "[18]\ttraining's binary_error: 0.0918659\tvalid_1's binary_error: 0.100562\n",
            "[19]\ttraining's binary_error: 0.0905734\tvalid_1's binary_error: 0.10025\n",
            "[20]\ttraining's binary_error: 0.0892808\tvalid_1's binary_error: 0.10025\n",
            "[21]\ttraining's binary_error: 0.0881728\tvalid_1's binary_error: 0.0990006\n",
            "[22]\ttraining's binary_error: 0.0868803\tvalid_1's binary_error: 0.098376\n",
            "[23]\ttraining's binary_error: 0.0866033\tvalid_1's binary_error: 0.0986883\n",
            "[24]\ttraining's binary_error: 0.0846644\tvalid_1's binary_error: 0.0986883\n",
            "[25]\ttraining's binary_error: 0.0835565\tvalid_1's binary_error: 0.0993129\n",
            "[26]\ttraining's binary_error: 0.0824485\tvalid_1's binary_error: 0.0990006\n",
            "[27]\ttraining's binary_error: 0.0816176\tvalid_1's binary_error: 0.0996252\n",
            "[28]\ttraining's binary_error: 0.080325\tvalid_1's binary_error: 0.0996252\n",
            "[29]\ttraining's binary_error: 0.0788478\tvalid_1's binary_error: 0.10025\n",
            "[30]\ttraining's binary_error: 0.0783861\tvalid_1's binary_error: 0.10025\n",
            "[31]\ttraining's binary_error: 0.0769089\tvalid_1's binary_error: 0.0996252\n",
            "[32]\ttraining's binary_error: 0.0758009\tvalid_1's binary_error: 0.0990006\n",
            "[33]\ttraining's binary_error: 0.0746007\tvalid_1's binary_error: 0.0993129\n",
            "[34]\ttraining's binary_error: 0.0735851\tvalid_1's binary_error: 0.0996252\n",
            "[35]\ttraining's binary_error: 0.0725695\tvalid_1's binary_error: 0.0990006\n",
            "[36]\ttraining's binary_error: 0.0711846\tvalid_1's binary_error: 0.0996252\n",
            "[37]\ttraining's binary_error: 0.0695227\tvalid_1's binary_error: 0.0993129\n",
            "[38]\ttraining's binary_error: 0.0689687\tvalid_1's binary_error: 0.0996252\n",
            "[39]\ttraining's binary_error: 0.0685994\tvalid_1's binary_error: 0.10025\n",
            "[40]\ttraining's binary_error: 0.0679531\tvalid_1's binary_error: 0.101499\n",
            "[41]\ttraining's binary_error: 0.0673991\tvalid_1's binary_error: 0.101187\n",
            "[42]\ttraining's binary_error: 0.0663835\tvalid_1's binary_error: 0.101187\n",
            "[43]\ttraining's binary_error: 0.0652756\tvalid_1's binary_error: 0.100562\n",
            "[44]\ttraining's binary_error: 0.0649063\tvalid_1's binary_error: 0.10025\n",
            "[45]\ttraining's binary_error: 0.0635214\tvalid_1's binary_error: 0.0999375\n",
            "[46]\ttraining's binary_error: 0.0628751\tvalid_1's binary_error: 0.10025\n",
            "[47]\ttraining's binary_error: 0.0629674\tvalid_1's binary_error: 0.10025\n",
            "[48]\ttraining's binary_error: 0.0617671\tvalid_1's binary_error: 0.10025\n",
            "[49]\ttraining's binary_error: 0.0613978\tvalid_1's binary_error: 0.0999375\n",
            "[50]\ttraining's binary_error: 0.0603822\tvalid_1's binary_error: 0.0999375\n",
            "[51]\ttraining's binary_error: 0.059182\tvalid_1's binary_error: 0.10025\n",
            "[52]\ttraining's binary_error: 0.0571508\tvalid_1's binary_error: 0.100562\n",
            "[53]\ttraining's binary_error: 0.0564122\tvalid_1's binary_error: 0.100874\n",
            "[54]\ttraining's binary_error: 0.0554889\tvalid_1's binary_error: 0.100874\n",
            "[55]\ttraining's binary_error: 0.0545656\tvalid_1's binary_error: 0.100562\n",
            "[56]\ttraining's binary_error: 0.053827\tvalid_1's binary_error: 0.100874\n",
            "[57]\ttraining's binary_error: 0.0525344\tvalid_1's binary_error: 0.100562\n",
            "[58]\ttraining's binary_error: 0.0521651\tvalid_1's binary_error: 0.100874\n",
            "[59]\ttraining's binary_error: 0.0506878\tvalid_1's binary_error: 0.0993129\n",
            "[60]\ttraining's binary_error: 0.0500415\tvalid_1's binary_error: 0.0993129\n",
            "[61]\ttraining's binary_error: 0.0492106\tvalid_1's binary_error: 0.0996252\n",
            "Early stopping, best iteration is:\n",
            "[11]\ttraining's binary_error: 0.10156\tvalid_1's binary_error: 0.098376\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bPicKRBxNrZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Evaluate the model on the test set ###\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_prob = model.predict(X_test, num_iteration=model.best_iteration)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# Calculate accuracy\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Feature Importance\n",
        "importance = model.feature_importance(importance_type='gain')\n",
        "feature_names = X_train.columns\n",
        "feature_importance = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
        "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Output results\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "print(\"Fraction of target that is positive (SEO happened):\", y_test.mean())\n",
        "print(\"Fraction of target that is negative (SEO did not happen):\", 1- y_test.mean())\n",
        "\n",
        "print(\"\\nFeature Importance:\")\n",
        "print(feature_importance[0:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obwmbTkssXMd",
        "outputId": "21a29e35-d6cb-4895-92b7-b45687dfed33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.8926489637305699\n",
            "\n",
            "Confusion Matrix:\n",
            "[[5482   24]\n",
            " [ 639   31]]\n",
            "Fraction of target that is positive (SEO happened): 0.10848445595854922\n",
            "Fraction of target that is negative (SEO did not happen): 0.8915155440414508\n",
            "\n",
            "Feature Importance:\n",
            "          Feature   Importance\n",
            "7      chcsho_12m  1824.975616\n",
            "1            ff49   613.058296\n",
            "10        ret_3_1   526.613196\n",
            "35     eqnetis_at   442.197598\n",
            "6       div12m_me   295.240095\n",
            "16       sale_gr1   269.575799\n",
            "32          gp_at   268.328805\n",
            "8       eqnpo_12m   257.016201\n",
            "40        opex_at   256.353605\n",
            "107      fincf_at   209.998903\n",
            "111      eqnpo_at   180.328502\n",
            "115       debt_at   177.145798\n",
            "42   sale_emp_gr1   149.202299\n",
            "11        ret_6_1   148.207598\n",
            "41        cash_at   147.824899\n",
            "106         ni_at   141.560498\n",
            "14      ret_60_12   117.190500\n",
            "2          dolvol   116.792100\n",
            "62        sga_gr1   104.387801\n",
            "114        nwc_at    93.626401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Report actual SEO fractions by buckets sorted on predicted probabilities (quintiles) ###\n",
        "\n",
        "test_results = pd.DataFrame({\n",
        "    'predicted_prob': y_pred_prob,\n",
        "    'actual_target': y_test.values\n",
        "})\n",
        "\n",
        "# Create quintiles based on predicted probabilities\n",
        "test_results['quintile'] = pd.qcut(test_results['predicted_prob'], q=5, labels=False)\n",
        "\n",
        "# Calculate the percentage of target == 1 in each quintile\n",
        "quintile_summary = test_results.groupby('quintile').apply(\n",
        "    lambda group: pd.Series({\n",
        "        'count': len(group),\n",
        "        'target_1_percentage': group['actual_target'].mean() * 100\n",
        "    }),\n",
        "    include_groups=False\n",
        ")\n",
        "\n",
        "# Display the results\n",
        "quintile_summary.columns = ['Count', 'Target == 1 Percentage (%)']\n",
        "print(\"\\nPercentage of target == 1 in each quintile:\")\n",
        "print(quintile_summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UEt7BuIXboc",
        "outputId": "231bb3ad-a459-46d7-cfe7-6ca92cf98535"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Percentage of target == 1 in each quintile:\n",
            "           Count  Target == 1 Percentage (%)\n",
            "quintile                                    \n",
            "0         1236.0                    2.508091\n",
            "1         1235.0                    3.562753\n",
            "2         1235.0                    7.125506\n",
            "3         1235.0                   11.174089\n",
            "4         1235.0                   29.878543\n"
          ]
        }
      ]
    }
  ]
}